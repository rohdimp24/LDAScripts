---
unicorn cases
```{r}

#this is the script that can connect to the mysql and then fetch the data
#install.packages("RMySQL")
library(RMySQL)
mydb = dbConnect(MySQL(), user='root', password='', dbname='newunicornplatformproject', host='localhost')

#this is the subset 
subcases<-dbGetQuery(mydb, "SELECT `CASE_ID`, `LAST_COMMUNICATION`, `PROBLEM_STATEMENT`, `CASE_HISTORY` FROM `casedetails` LIMIT 500")

library(tm)
library(SnowballC)
library(Matrix)
library(lda)
library(LDAvis)
library(servr)


txt=subcases$CASE_HISTORY

# pre-processing
txt <- gsub("'", "", txt)  # remove apostrophes
txt <- gsub("[[:punct:]]", " ", txt)  # replace punctuation with space
txt <- gsub("[[:cntrl:]]", " ", txt)  # replace control characters with space
txt <- gsub("^[[:space:]]+", "", txt) # remove whitespace at beginning of documents
txt <- gsub("[[:space:]]+$", "", txt) # remove whitespace at end of documents
#txt <- tolower(txt)  # force to lowercase

# tokenize on space and output as a list:
doc.list <- strsplit(txt, "[[:space:]]+")

term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)


# stop words
stop_words = c(1:10, stopwords("english"), LETTERS)
stop_words=c(stop_words,"Healthcare" ,"GE","Royne","Eriksson","Ã","Masaru","Daniel","Asano","Engineer","Kerstin","AM","Case","IQ","Adam","Berlex","Hagberg","Kaletski","March","Referral","Regards","Steven","Dan","PM","CSSX,February","Field","History","Marcus,Read","Specialist","Tuesday,Uppsala,Cambrex","CSSx","Dec","Feb","Fixed","Greg","Joakim","JPG","Karlsson","Kjell","Schiedt","Stefan,Screen","Andreas","Bergstrom","Bjorn","Bo","Christian","Date","Eric","Friday","Irina","KTA","Monday","Sannervik,Steve","Sweden","Haberstroh","Michael","Address","Bjon","California","Co","Cohen","Dahl","Does","Explain","FROM","Gateway","HP","Inc","Jaderlund","Jan","January","Johan","Johansson","LaBreck","Lamonica","Manager","Marcus","Mantovaara","Mathov","May","Mike","Milbourn","MotorB","Northeast","Nov","October","Offâ","Ohlin","Oishi","Petrell","Product","Raymond","Report","Sara","Simon","Soderstrom,Support","SUPPORT","SW","SWEDISH","TEAM","Thom","Truman","Tuula","UF","Urban","US","WAIT","Win","Yi","January","january","feburary","Feburary","march","april","may","june","july","august","september","october","november","december","March","April","May","June","July","August","September","October","November","December","Karlberg","Nakamura","Niklas","Schwald","Frieder","â","Hochberger","Taylor","LaMonica","Lars","Mr","Mr.","Shafiq","Vikram","Alexander","Galin","Rohit","Agarwal","Winther","Aug","Sep","Apr","Mar","Jul","Stefan","Pontoppidan","Jun","Ursula","Christer","Olivier","Schwicker","Oct","Friker","Frank","Gut","Markus","de","Time","Werner","John","Jon","Les","George","Uppsala","Baker","Linda","Im","Landby","Walter","Ill","Jing","Friedl","Krasnansky","Microsoft","Tommy","Hideaki","Tomas","V1","Annica","Johann","Otsuka","Anneli","Carlos","Debbie","HÃ","Lutz","ã","BjÃ","Gebauer","Nevil","Ive","Fredrik","Gerard","Tack","Liu","Lot","Lots","Pettersson","Susanne","Bjornsson","Data","Oskar","Martin","AA","Cannavale","Europe","Koen","Lundquist","Marzia","Thomas","AB","Banzawa","FW","Michaloudis","Niebler","Lundgren","MB","Alessandro","ASAP","Chelating","Mathe","Takako","Lindgren","Deane","Declan","February","Harald","Juri","Nicklas","Ning","Norbitec","Task","Thorell","Hi","Vercammen","Akervall","Anna","Bettin","Brassard","Erik","Giacomo","Katsumoto","Kim","Sheila","Swedberg","Stijn","Camilla","des","Healy","Larsson","nSatoshi","Olle","Peti","Subject","To","From","Hi")


# remove terms that are stop words or occur fewer than 3 times:
del <- names(term.table) %in% stop_words | term.table < 3
term.table <- term.table[!del]
vocab <- names(term.table)

```

The NLP is used to find the proper nouns. These are the words that will be some persons name or some common words. We need to add them to the list of the stop words. This will reduce the size of the case data to work with
```{r}



library("openNLP")
## Some text.

s<-unlist(vocab)
s <- as.String(s)

## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(s, pos_tag_annotator, a2)
a3
## Variant with POS tag probabilities as (additional) features.
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))

#from http://www.inside-r.org/packages/cran/openNLP/docs/Maxent_POS_Tag_Annotator
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
#sprintf("%s/%s", s[a3w], tags)
#to displayu only the nouns
tt<-s[a3w[which(tags=="NNP")]]

#to get the comma seperated lis that can be used in the notepad to do some manuall inspection
paste(tt,collapse = ",")



```
