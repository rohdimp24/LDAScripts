---
unicorn cases
```{r}

#this is the script that can connect to the mysql and then fetch the data
#install.packages("RMySQL")
library(RMySQL)
mydb = dbConnect(MySQL(), user='root', password='', dbname='newunicornplatformproject', host='localhost')
cases <- dbGetQuery(mydb, "SELECT `CASE_ID`, `LAST_COMMUNICATION`, `PROBLEM_STATEMENT`, `CASE_HISTORY` FROM `casedetails`")

#this is the subset 
subcases<-dbGetQuery(mydb, "SELECT `CASE_ID`, `LAST_COMMUNICATION`, `PROBLEM_STATEMENT`, `CASE_HISTORY` FROM `casedetails` LIMIT 500")

library(tm)
library(SnowballC)
library(Matrix)
library(lda)
library(LDAvis)
library(servr)


txt = cases$CASE_HISTORY
txt=subcases

# pre-processing
txt <- gsub("'", "", txt)  # remove apostrophes
txt <- gsub("[[:punct:]]", " ", txt)  # replace punctuation with space
txt <- gsub("[[:cntrl:]]", " ", txt)  # replace control characters with space
txt <- gsub("^[[:space:]]+", "", txt) # remove whitespace at beginning of documents
txt <- gsub("[[:space:]]+$", "", txt) # remove whitespace at end of documents
#txt <- tolower(txt)  # force to lowercase

# tokenize on space and output as a list:
doc.list <- strsplit(txt, "[[:space:]]+")

term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)


# stop words
stop_words = c(1:10, stopwords("english"), LETTERS)
#updated the list of stop words
stop_words=c(stop_words,"Healthcare" ,"GE","Royne","Eriksson","Ã","Masaru","Daniel","Asano","Engineer","Kerstin","AM","Case","IQ","Adam","Berlex","Hagberg","Kaletski","March","Referral","Regards","Steven","Dan","PM","CSSX,February","Field","History","Marcus,Read","Specialist","Tuesday,Uppsala,Cambrex","CSSx","Dec","Feb","Fixed","Greg","Joakim","JPG","Karlsson","Kjell","Schiedt","Stefan,Screen","Andreas","Bergstrom","Bjorn","Bo","Christian","Date","Eric","Friday","Irina","KTA","Monday","Sannervik,Steve","Sweden","Haberstroh","Michael","Address","Bjon","California","Co","Cohen","Dahl","Does","Explain","FROM","Gateway","HP","Inc","Jaderlund","Jan","January","Johan","Johansson","LaBreck","Lamonica","Manager","Marcus","Mantovaara","Mathov","May","Mike","Milbourn","MotorB","Northeast","Nov","October","Offâ","Ohlin","Oishi","Petrell","Product","Raymond","Report","Sara","Simon","Soderstrom,Support","SUPPORT","SW","SWEDISH","TEAM","Thom","Truman","Tuula","UF","Urban","US","WAIT","Win","Yi","January","january","feburary","Feburary","march","april","may","june","july","august","september","october","november","december","March","April","May","June","July","August","September","October","November","December","Karlberg","Nakamura","Niklas","Schwald","Frieder","â","Hochberger","Taylor","LaMonica","Lars","Mr","Mr.","Shafiq","Vikram","Alexander","Galin","Rohit","Agarwal","Winther","Aug","Sep","Apr","Mar","Jul","Stefan","Pontoppidan","Jun","Ursula","Christer","Olivier","Schwicker","Oct","Friker","Frank","Gut","Markus","de","Time","Werner","John","Jon","Les","George","Uppsala","Baker","Linda","Im","Landby","Walter","Ill","Jing","Friedl","Krasnansky","Microsoft","Tommy","Hideaki","Tomas","V1","Annica","Johann","Otsuka","Anneli","Carlos","Debbie","HÃ","Lutz","ã","BjÃ","Gebauer","Nevil","Ive","Fredrik","Gerard","Tack","Liu","Lot","Lots","Pettersson","Susanne","Bjornsson","Data","Oskar","Martin","AA","Cannavale","Europe","Koen","Lundquist","Marzia","Thomas","AB","Banzawa","FW","Michaloudis","Niebler","Lundgren","MB","Alessandro","ASAP","Chelating","Mathe","Takako","Lindgren","Deane","Declan","February","Harald","Juri","Nicklas","Ning","Norbitec","Task","Thorell","Hi","Vercammen","Akervall","Anna","Bettin","Brassard","Erik","Giacomo","Katsumoto","Kim","Sheila","Swedberg","Stijn","Camilla","des","Healy","Larsson","nSatoshi","Olle","Peti")


# remove terms that are stop words or occur fewer than 3 times:
del <- names(term.table) %in% stop_words | term.table < 3
term.table <- term.table[!del]
vocab <- names(term.table)

performNLP(vocab)

```
Performing LDA. 
Converting the data into the data structures understood by lda(). There is another library called topic modelling, which can take the document term matrix as an input instead of special data strcuture as in case of lda()

```{r}

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)
W <- length(vocab)  
doc.length <- sapply(documents, function(x) sum(x[2, ])) 
N <- sum(doc.length)  
term.frequency <- as.integer(term.table)

K <- 30
G <- 1000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  

theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))

```
Drawing the visualization

```{r}

#colnames(theta)<-c(1:30)

results <- list(phi = phi,
                theta = theta,
                doc.length = doc.length,
                vocab = vocab,
                term.frequency = term.frequency)

json <- createJSON(phi = results$phi, 
                   theta = results$theta, 
                   doc.length = results$doc.length, 
                   vocab = results$vocab, 
                   term.frequency = results$term.frequency)
write(json, file="export_small.JSON")


serVis(json, out.dir = 'vis2', open.browser = FALSE)

#note that the ldavis is changing the topic order
#"topic.order": [ 24, 27, 13, 21, 9, 23, 16, 7, 11, 22, 4, 17, 12, 3, 14, 20, 5, 2, 30, 28, 29, 26, 1, 8, 10, 15, 18, 25, 6, 19 ]
#this means the topic 19 on LDAvis is actually topic 30 in theta

library(rjson)
jsonDocument <- fromJSON(json_str = json)

ldavisTopicOrder=as.vector(jsonDocument$topic.order)

#since LDAvis will change the order of the topics while displaying so this will map the ldavis topic number with the theta matrix topic number
getThetaIndexForTopic=function(topicNumber){
  index=1
  for(i in 1:length(ldavisTopicOrder)){
    if(ldavisTopicOrder[i]==topicNumber){
        index=i
        break;    
    }
  }
  index  
}


```
Now trying to verifying what we  see in the LDAvis is mapping to the theta table as welll

```{r, echo=FALSE}

topic_doc=theta

#these are the top words in 
top.words <- top.topic.words(fit$topics, 10, by.score=TRUE)
#this will give a matrix of number of wordsxK where each column will correspond to the top words for the topic

#indexes of the documents conatainign the tipoic
index=getThetaIndexForTopic(21)
index
gg=which(topic_doc[,index]>0.1)
subcases$CASE_HISTORY[gg[1]]
#key words for this topic
top.words[,index]

```


The NLP is used to find the proper nouns. These are the words that will be some persons name or some common words. We need to add them to the list of the stop words. This will reduce the size of the case data to work with
```{r}

performNLP=function(vocab){
  
  library("openNLP")
  ## Some text.
  s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
  "nonexecutive director Nov. 29.\n",
  "Mr. Vinken is chairman of Elsevier N.V., ",
  "the Dutch publishing group."),
  collapse = "")
  
  #s<-unlist(subcases)
  s<-unlist(vocab)
  s <- as.String(s)
  ## Need sentence and word token annotations.
  sent_token_annotator <- Maxent_Sent_Token_Annotator()
  word_token_annotator <- Maxent_Word_Token_Annotator()
  a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
  pos_tag_annotator <- Maxent_POS_Tag_Annotator()
  pos_tag_annotator
  a3 <- annotate(s, pos_tag_annotator, a2)
  a3
  ## Variant with POS tag probabilities as (additional) features.
  head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
  
  #from http://www.inside-r.org/packages/cran/openNLP/docs/Maxent_POS_Tag_Annotator
  a3w <- subset(a3, type == "word")
  tags <- sapply(a3w$features, `[[`, "POS")
  tags
  table(tags)
  sprintf("%s/%s", s[a3w], tags)
  
  #to displayu only the nouns
  tt<-s[a3w[which(tags=="NNP")]]
  paste(tt,collapse = ",")
  #tt[which(tt!="_")]
  #need to perform the manual task of findign the names of the people from the tagged list. Add them    #as stop words
}

```
